import os, torch
from tqdm import tqdm
from accelerate import Accelerator
from .training_module import DiffusionTrainingModule
from .logger import ModelLogger
import torch
from tqdm import tqdm
import time
import time
from contextlib import contextmanager
from collections import defaultdict

import time
from collections import defaultdict
from contextlib import contextmanager
import statistics
import os
import torch
from datetime import datetime

import time
import torch
import torch.distributed as dist
from contextlib import contextmanager
from collections import defaultdict
from datetime import datetime
import statistics


class DetailedStepTimer:
    """
    é’ˆå¯¹å¤šæœºå¤šå¡è®­ç»ƒçš„è¯¦ç»†è®¡æ—¶å™¨
    
    åŠŸèƒ½ï¼š
    1. ä½¿ç”¨ CUDA Events ç²¾ç¡®æµ‹é‡ GPU æ—¶é—´ï¼ˆé¿å… CPU-GPU å¼‚æ­¥è¯¯å·®ï¼‰
    2. åˆ†ç¦» backward çº¯è®¡ç®— vs æ¢¯åº¦åŒæ­¥æ—¶é—´
    3. æµ‹é‡è¿›ç¨‹é—´ç­‰å¾…/è´Ÿè½½ä¸å‡è¡¡
    4. æ”¯æŒæ¯ä¸ª rank ç‹¬ç«‹è®°å½•ï¼Œæœ€åæ±‡æ€»æ¯”è¾ƒ
    """
    
    def __init__(self, log_file="training_perf_detailed.log", warmup_steps=2):
        self.times = defaultdict(list)
        self.step_keys = []
        self.log_file = log_file
        self.warmup_steps = warmup_steps  # å‰å‡ æ­¥ä¸è®¡å…¥ç»Ÿè®¡ï¼ˆé¢„çƒ­ï¼‰
        self.current_step = 0
        
        self.use_cuda = torch.cuda.is_available()
        
        # æ¯ä¸ª rank çš„ç‹¬ç«‹æ—¶é—´è®°å½•ï¼ˆç”¨äºåˆ†æè´Ÿè½½ä¸å‡è¡¡ï¼‰
        self.per_rank_times = defaultdict(list)
    
    @contextmanager
    def time_step(self, name):
        """
        ä½¿ç”¨ CUDA Events çš„ç²¾ç¡® GPU è®¡æ—¶
        - CUDA Events ç›´æ¥åœ¨ GPU ä¸Šè®°å½•æ—¶é—´æˆ³ï¼Œé¿å… CPU-GPU å¼‚æ­¥å¸¦æ¥çš„è¯¯å·®
        """
        if name not in self.step_keys:
            self.step_keys.append(name)
        
        if self.use_cuda:
            start_event = torch.cuda.Event(enable_timing=True)
            end_event = torch.cuda.Event(enable_timing=True)
            
            # åŒæ­¥ç¡®ä¿ä¹‹å‰çš„æ“ä½œå®Œæˆ
            torch.cuda.synchronize()
            start_event.record()
            
            yield
            
            end_event.record()
            torch.cuda.synchronize()
            
            # elapsed_time è¿”å›æ¯«ç§’ï¼Œè½¬ä¸ºç§’
            elapsed = start_event.elapsed_time(end_event) / 1000.0
        else:
            start = time.perf_counter()
            yield
            elapsed = time.perf_counter() - start
        
        self.times[name].append(elapsed)
    
    @contextmanager
    def time_step_no_sync(self, name):
        """
        ä¸å¸¦ GPU åŒæ­¥çš„è®¡æ—¶ï¼ˆç”¨äºæµ‹é‡ CPU ç«¯æ“ä½œæˆ–æ•…æ„ä¸åŒæ­¥çš„åœºæ™¯ï¼‰
        """
        if name not in self.step_keys:
            self.step_keys.append(name)
        
        start = time.perf_counter()
        yield
        elapsed = time.perf_counter() - start
        self.times[name].append(elapsed)
    
    def record(self, name, elapsed):
        """æ‰‹åŠ¨è®°å½•æ—¶é—´"""
        if name not in self.step_keys:
            self.step_keys.append(name)
        self.times[name].append(elapsed)
    
    def measure_load_imbalance(self, accelerator, name="load_imbalance"):
        """
        æµ‹é‡è¿›ç¨‹é—´çš„è´Ÿè½½ä¸å‡è¡¡ï¼ˆbarrier ç­‰å¾…æ—¶é—´ï¼‰
        
        åŸç†ï¼šæ¯ä¸ªè¿›ç¨‹åˆ°è¾¾ barrier çš„æ—¶é—´ä¸åŒï¼Œå…ˆåˆ°çš„è¦ç­‰ååˆ°çš„
        è¿™ä¸ªç­‰å¾…æ—¶é—´åæ˜ äº†å„ GPU è®¡ç®—é€Ÿåº¦çš„å·®å¼‚
        """
        if name not in self.step_keys:
            self.step_keys.append(name)
        
        if self.use_cuda:
            torch.cuda.synchronize()
        
        start = time.perf_counter()
        
        if dist.is_initialized():
            dist.barrier()
        
        elapsed = time.perf_counter() - start
        self.times[name].append(elapsed)
        return elapsed
    
    def time_backward_separated(self, accelerator, model, loss):
        """
        åˆ†ç¦»æµ‹é‡ backward çš„å„ä¸ªé˜¶æ®µï¼š
        1. backward_compute: çº¯åå‘ä¼ æ’­è®¡ç®—
        2. gradient_sync: æ¢¯åº¦ AllReduce åŒæ­¥
        
        å…³é”®æŠ€å·§ï¼šä½¿ç”¨ accelerator.no_sync() æ¥é˜»æ­¢è‡ªåŠ¨æ¢¯åº¦åŒæ­¥
        """
        for name in ["backward_compute", "gradient_sync", "backward_total"]:
            if name not in self.step_keys:
                self.step_keys.append(name)
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥åŒæ­¥æ¢¯åº¦ï¼ˆæ¢¯åº¦ç´¯ç§¯çš„æœ€åä¸€æ­¥ï¼‰
        should_sync = accelerator.sync_gradients
        
        if self.use_cuda:
            torch.cuda.synchronize()
        
        total_start = time.perf_counter()
        
        if should_sync:
            # === æ–¹æ³•1ï¼šå¦‚æœæ˜¯åŒæ­¥æ­¥ï¼Œå°è¯•åˆ†ç¦»è®¡ç®—å’Œé€šä¿¡ ===
            
            # å…ˆç”¨ no_sync æ‰§è¡Œ backwardï¼ˆåªè®¡ç®—ï¼Œä¸åŒæ­¥ï¼‰
            compute_start = time.perf_counter()
            
            # ä½¿ç”¨ no_sync ä¸Šä¸‹æ–‡
            with accelerator.no_sync(model):
                accelerator.backward(loss)
            
            if self.use_cuda:
                torch.cuda.synchronize()
            compute_time = time.perf_counter() - compute_start
            
            # æ‰‹åŠ¨è§¦å‘æ¢¯åº¦åŒæ­¥
            sync_start = time.perf_counter()
            
            # å¯¹äº DDPï¼Œæ‰‹åŠ¨æ‰§è¡Œ allreduce
            if hasattr(accelerator, 'reducer') or hasattr(model, 'reducer'):
                # æŸäº›æƒ…å†µä¸‹éœ€è¦æ‰‹åŠ¨è§¦å‘
                pass  # DDP ä¼šåœ¨ä¸‹æ¬¡ forward æ—¶è‡ªåŠ¨åŒæ­¥ï¼Œæˆ–ä½¿ç”¨ä¸‹é¢çš„æ–¹æ³•
            
            # ä½¿ç”¨ distributed åŸè¯­æ‰‹åŠ¨åŒæ­¥ï¼ˆæ›´ç²¾ç¡®ä½†éœ€è¦å°å¿ƒå¤„ç†ï¼‰
            if dist.is_initialized() and accelerator.num_processes > 1:
                # è·å–æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦å¹¶æ‰§è¡Œ allreduce
                for param in model.parameters():
                    if param.grad is not None:
                        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
            
            if self.use_cuda:
                torch.cuda.synchronize()
            sync_time = time.perf_counter() - sync_start
            
        else:
            # === æ¢¯åº¦ç´¯ç§¯ä¸­é—´æ­¥ï¼Œä¸éœ€è¦åŒæ­¥ ===
            compute_start = time.perf_counter()
            accelerator.backward(loss)
            
            if self.use_cuda:
                torch.cuda.synchronize()
            
            compute_time = time.perf_counter() - compute_start
            sync_time = 0.0
        
        total_time = time.perf_counter() - total_start
        
        self.times["backward_compute"].append(compute_time)
        self.times["gradient_sync"].append(sync_time)
        self.times["backward_total"].append(total_time)
        
        return compute_time, sync_time
    
    def time_backward_with_profiler(self, accelerator, loss, profile_memory=False):
        """
        ä½¿ç”¨ PyTorch Profiler æ¥åˆ†æ backward
        è¿™æ˜¯æœ€ç²¾ç¡®çš„æ–¹æ³•ï¼Œå¯ä»¥çœ‹åˆ° NCCL é€šä¿¡çš„è¯¦ç»†æ—¶é—´
        """
        if "backward_profiled" not in self.step_keys:
            self.step_keys.append("backward_profiled")
        
        activities = [torch.profiler.ProfilerActivity.CPU]
        if self.use_cuda:
            activities.append(torch.profiler.ProfilerActivity.CUDA)
        
        with torch.profiler.profile(
            activities=activities,
            record_shapes=True,
            profile_memory=profile_memory,
            with_stack=True
        ) as prof:
            if self.use_cuda:
                torch.cuda.synchronize()
            start = time.perf_counter()
            
            accelerator.backward(loss)
            
            if self.use_cuda:
                torch.cuda.synchronize()
            elapsed = time.perf_counter() - start
        
        self.times["backward_profiled"].append(elapsed)
        
        return prof  # è¿”å› profiler å¯¹è±¡ï¼Œå¯ä»¥è¿›ä¸€æ­¥åˆ†æ
    
    def collect_all_ranks(self, accelerator):
        """
        æ”¶é›†æ‰€æœ‰ rank çš„è®¡æ—¶æ•°æ®åˆ°ä¸»è¿›ç¨‹
        ç”¨äºåˆ†æå„ GPU çš„è´Ÿè½½å·®å¼‚
        """
        if not dist.is_initialized():
            return
        
        world_size = accelerator.num_processes
        rank = accelerator.process_index
        
        # å°†æœ¬è¿›ç¨‹çš„æ•°æ®è½¬ä¸º tensor
        all_rank_data = {}
        
        for key in self.step_keys:
            local_times = torch.tensor(self.times[key], dtype=torch.float32)
            if self.use_cuda:
                local_times = local_times.cuda()
            
            # æ”¶é›†æ‰€æœ‰ rank çš„æ•°æ®
            gathered = [torch.zeros_like(local_times) for _ in range(world_size)]
            dist.all_gather(gathered, local_times)
            
            all_rank_data[key] = [t.cpu().numpy() for t in gathered]
        
        self.all_rank_data = all_rank_data
    
    def print_summary(self, accelerator, show_per_rank=True):
        """æ‰“å°è¯¦ç»†çš„æ€§èƒ½æ‘˜è¦"""
        if not accelerator.is_main_process:
            return
        
        if not self.step_keys:
            return
        
        # è·³è¿‡é¢„çƒ­æ­¥
        effective_times = {
            k: v[self.warmup_steps:] if len(v) > self.warmup_steps else v
            for k, v in self.times.items()
        }
        
        num_steps = max(len(effective_times[k]) for k in self.step_keys) if self.step_keys else 0
        
        output = []
        output.append("\n" + "=" * 140)
        output.append(f"{'è¯¦ç»†æ€§èƒ½åˆ†ææŠ¥å‘Š - ' + datetime.now().strftime('%Y-%m-%d %H:%M:%S'):^140s}")
        output.append(f"{'(è·³è¿‡å‰ ' + str(self.warmup_steps) + ' æ­¥é¢„çƒ­)':^140s}")
        output.append("=" * 140)
        
        # === 1. æ¯æ­¥è¯¦æƒ… ===
        output.append("\nã€æ¯æ­¥è€—æ—¶è¯¦æƒ… (ms)ã€‘")
        headers = ["Step"] + self.step_keys + ["Total"]
        col_width = max(15, max(len(h) for h in headers) + 2)
        header_str = "".join([f"{h:>{col_width}s}" for h in headers])
        output.append(header_str)
        output.append("-" * len(header_str))
        
        for i in range(num_steps):
            row_vals = []
            step_total = 0.0
            for key in self.step_keys:
                vals = effective_times.get(key, [])
                if i < len(vals):
                    val = vals[i] * 1000  # è½¬ä¸º ms
                else:
                    val = 0.0
                step_total += val
                row_vals.append(f"{val:{col_width}.2f}")
            
            row_str = f"{i:>{col_width}d}" + "".join(row_vals) + f"{step_total:{col_width}.2f}"
            output.append(row_str)
        
        # === 2. ç»Ÿè®¡æ‘˜è¦ ===
        output.append("\n" + "=" * 140)
        output.append(f"{'ç»Ÿè®¡æ‘˜è¦':^140s}")
        output.append("=" * 140)
        
        total_time_all = sum(sum(v) for v in effective_times.values())
        
        stats_header = f"{'é˜¶æ®µ':<25s} | {'å¹³å‡(ms)':>12s} | {'æ ‡å‡†å·®(ms)':>12s} | {'æœ€å°(ms)':>12s} | {'æœ€å¤§(ms)':>12s} | {'æ€»è®¡(s)':>12s} | {'å æ¯”':>8s}"
        output.append(stats_header)
        output.append("-" * len(stats_header))
        
        for name in self.step_keys:
            values = effective_times.get(name, [])
            if values and any(v > 0 for v in values):
                # è¿‡æ»¤æ‰ 0 å€¼è¿›è¡Œç»Ÿè®¡ï¼ˆé’ˆå¯¹æ¢¯åº¦ç´¯ç§¯åœºæ™¯ï¼‰
                non_zero_values = [v for v in values if v > 0]
                if non_zero_values:
                    avg = statistics.mean(non_zero_values) * 1000
                    std = statistics.stdev(non_zero_values) * 1000 if len(non_zero_values) > 1 else 0
                    min_val = min(non_zero_values) * 1000
                    max_val = max(non_zero_values) * 1000
                    total = sum(values)
                    ratio = (total / total_time_all) * 100 if total_time_all > 0 else 0
                    
                    output.append(
                        f"{name:<25s} | {avg:>12.2f} | {std:>12.2f} | {min_val:>12.2f} | "
                        f"{max_val:>12.2f} | {total:>12.2f} | {ratio:>7.1f}%"
                    )
        
        # === 3. æ€§èƒ½ç“¶é¢ˆåˆ†æ ===
        output.append("\n" + "=" * 140)
        output.append(f"{'ç“¶é¢ˆåˆ†æ':^140s}")
        output.append("=" * 140)
        
        # æ‰¾å‡ºå æ¯”æœ€é«˜çš„é˜¶æ®µ
        ratios = {}
        for name in self.step_keys:
            values = effective_times.get(name, [])
            if values:
                total = sum(values)
                ratios[name] = total / total_time_all if total_time_all > 0 else 0
        
        sorted_ratios = sorted(ratios.items(), key=lambda x: x[1], reverse=True)
        
        output.append("è€—æ—¶æ’å (ä»é«˜åˆ°ä½):")
        for i, (name, ratio) in enumerate(sorted_ratios[:5], 1):
            bar_len = int(ratio * 50)
            bar = "â–ˆ" * bar_len + "â–‘" * (50 - bar_len)
            output.append(f"  {i}. {name:<20s} [{bar}] {ratio*100:.1f}%")
        
        # === 4. å»ºè®® ===
        output.append("\nã€ä¼˜åŒ–å»ºè®®ã€‘")
        if "data_loading" in ratios and ratios.get("data_loading", 0) > 0.2:
            output.append("  âš ï¸  æ•°æ®åŠ è½½å æ¯”è¾ƒé«˜ (>20%)ï¼Œå»ºè®®ï¼šå¢åŠ  num_workersã€ä½¿ç”¨ pin_memoryã€é¢„å–æ•°æ®")
        
        if "gradient_sync" in ratios and ratios.get("gradient_sync", 0) > 0.3:
            output.append("  âš ï¸  æ¢¯åº¦åŒæ­¥å æ¯”è¾ƒé«˜ (>30%)ï¼Œå»ºè®®ï¼šå¢å¤§ batch sizeã€ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ã€æ£€æŸ¥ç½‘ç»œå¸¦å®½")
        
        if "load_imbalance" in ratios and ratios.get("load_imbalance", 0) > 0.1:
            output.append("  âš ï¸  è´Ÿè½½ä¸å‡è¡¡ä¸¥é‡ (>10%)ï¼Œå»ºè®®ï¼šæ£€æŸ¥å„ GPU çš„æ•°æ®åˆ†å¸ƒæ˜¯å¦å‡åŒ€")
        
        forward_time = ratios.get("forward", 0)
        backward_time = ratios.get("backward_compute", ratios.get("backward", 0))
        if backward_time > 0 and forward_time > 0:
            ratio_fb = backward_time / forward_time
            output.append(f"  ğŸ“Š Backward/Forward æ¯”å€¼: {ratio_fb:.2f} (æ­£å¸¸èŒƒå›´: 2.0-3.0)")
        
        final_log = "\n".join(output)
        
        print(final_log)
        
        with open(self.log_file, "a", encoding="utf-8") as f:
            f.write(final_log + "\n")
        print("completed!")



class StepTimer:
    def __init__(self, log_file="training_perf.log"):
        self.times = defaultdict(list)
        self.step_keys = []
        self.log_file = log_file

    @contextmanager
    def time_step(self, name):
        if name not in self.step_keys:
            self.step_keys.append(name)
        # ç¡®ä¿ GPU åŒæ­¥ï¼Œå¦åˆ™æ—¶é—´ç»Ÿè®¡ä¸å‡†
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        start = time.perf_counter()
        yield
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        elapsed = time.perf_counter() - start
        self.times[name].append(elapsed)

    def record(self, name, elapsed):
        if name not in self.step_keys:
            self.step_keys.append(name)
        self.times[name].append(elapsed)

    def print_summary(self, accelerator):
        # ä»…åœ¨ä¸»è¿›ç¨‹ä¸­æ‰§è¡Œæ‰“å°å’Œå†™æ–‡ä»¶
        if not accelerator.is_main_process:
            return

        if not self.step_keys:
            return
            
        # ä»¥è®°å½•æœ€å¤šçš„ key ä¸ºå‡†ï¼ˆé€šå¸¸æ˜¯ data_loadingï¼‰
        num_steps = max(len(self.times[k]) for k in self.step_keys)
        
        output = []
        output.append("\n" + "="*120)
        output.append(f"{'Step è€—æ—¶è¯¦æƒ… (å•ä½: ms) - ' + datetime.now().strftime('%Y-%m-%d %H:%M:%S'):^120s}")
        output.append("="*120)
        
        headers = ["Step"] + self.step_keys + ["Total"]
        col_width = 15 
        header_str = "".join([f"{h:>{col_width}s}" for h in headers])
        output.append(header_str)
        output.append("-" * len(header_str))

        for i in range(num_steps):
            row_vals = []
            step_total = 0.0
            for key in self.step_keys:
                # å¥å£®æ€§å¤„ç†ï¼šå¦‚æœæŸé¡¹æ²¡æœ‰è®°å½•ï¼ˆæ¯”å¦‚æ¢¯åº¦ç´¯ç§¯è·³è¿‡äº†ï¼‰ï¼Œè®°ä¸º 0
                if i < len(self.times[key]):
                    val = self.times[key][i] * 1000
                else:
                    val = 0.0
                step_total += val
                row_vals.append(f"{val:{col_width}.2f}")
            
            row_str = f"{i:>{col_width}d}" + "".join(row_vals) + f"{step_total:{col_width}.2f}"
            output.append(row_str)

        # ç»Ÿè®¡æ‘˜è¦
        output.append("\n" + "="*120)
        output.append(f"{'ç»Ÿè®¡æ‘˜è¦ (å¹³å‡å€¼)':^120s}")
        output.append("="*120)
        total_time_all_steps = sum(sum(v) for v in self.times.values())
        
        for name in self.step_keys:
            values = self.times[name]
            if values:
                avg = statistics.mean(values) * 1000
                total = sum(values)
                ratio = (total / total_time_all_steps) * 100 if total_time_all_steps > 0 else 0
                output.append(f"{name:<25s} | å¹³å‡: {avg:10.2f} ms | æ€»è®¡: {total:10.2f} s | å æ¯”: {ratio:7.1f}%")
        
        final_log = "\n".join(output)
        
        # 1. æ‰“å°åˆ°ç»ˆç«¯
        print(final_log)
        
        # 2. ä¿å­˜åˆ°æ–‡ä»¶
        with open(self.log_file, "a", encoding="utf-8") as f:
            f.write(final_log + "\n")

def diagnose_default_training_status(model):
    """
    è¯Šæ–­æ¨¡å‹å½“å‰çš„é»˜è®¤è®­ç»ƒçŠ¶æ€ï¼ˆåœ¨äººå·¥ä¿®æ”¹ requires_grad ä¹‹å‰ï¼‰ï¼Œdebugæ¨¡å¼ä¸‹æ‰è¾“å‡º
    """
    print("\n" + "="*50)
    print("ğŸ•µï¸ [è¯Šæ–­æ¨¡å¼] æ£€æŸ¥æ¨¡å‹é»˜è®¤è®­ç»ƒçŠ¶æ€...")
    print("="*50)
    
    trainable_params = []
    frozen_params = []
    
    trainable_numel = 0
    frozen_numel = 0
    
    for name, param in model.named_parameters():
        if param.requires_grad:
            trainable_params.append(name)
            trainable_numel += param.numel()
        else:
            frozen_params.append(name)
            frozen_numel += param.numel()
            
    # ç»Ÿè®¡æ•°æ®
    total_layers = len(trainable_params) + len(frozen_params)
    total_params = trainable_numel + frozen_numel
    
    print(f"ğŸ“Š ç»Ÿè®¡ç»“æœ:")
    print(f"   - æ€»å±‚æ•° (Keys): {total_layers}")
    print(f"   - æ€»å‚æ•°é‡ (Elements): {total_params / 1e9:.2f} B (åäº¿)")
    print(f"   -------------------------------------------")
    print(f"   ğŸ”“ å¯è®­ç»ƒå±‚æ•° (Trainable): {len(trainable_params)}")
    print(f"      - å‚æ•°é‡: {trainable_numel / 1e9:.2f} B")
    print(f"      - å æ¯”: {trainable_numel / total_params * 100:.2f}%")
    print(f"   ğŸ”’ ä¸å¯è®­ç»ƒå±‚æ•° (Frozen): {len(frozen_params)}")
    print(f"      - å‚æ•°é‡: {frozen_numel / 1e9:.2f} B")
    print(f"   -------------------------------------------")
    
    # æ‰“å°å…·ä½“åå­—ï¼ˆä¸ºäº†é˜²æ­¢åˆ·å±ï¼Œæ¯ç§åªæ‰“å°å‰5ä¸ªå’Œå5ä¸ªï¼‰
    if len(trainable_params) > 0:
        print(f"\nğŸ“ å¯è®­ç»ƒå‚æ•°ç¤ºä¾‹ (Top 5):")
        for p in trainable_params[:10]:
            print(f"   - [âˆš] {p}")
        if len(trainable_params) > 10: print("   ... (ä¸­é—´çœç•¥) ...")
        # æ‰“å°æœ€åå‡ ä¸ªï¼Œçœ‹çœ‹éŸ³é¢‘éƒ¨åˆ†åœ¨ä¸åœ¨
        for p in trainable_params[-10:]:
            print(f"   - [âˆš] {p}")
            
    if len(frozen_params) > 0:
        print(f"\nğŸ§Š ä¸å¯è®­ç»ƒå‚æ•°ç¤ºä¾‹ (Top 5):")
        for p in frozen_params[:10]:
            print(f"   - [x] {p}")
            
    print("="*50 + "\n")


def prepare_model_and_optimizer_groups(model, base_lr=1e-5, target_lr=1e-4):
    # 1. å®šä¹‰é«˜å­¦ä¹ ç‡ï¼ˆä¸”éœ€è¦ç½®é›¶ï¼‰çš„ç›®æ ‡æ¨¡å—å‰ç¼€
    target_prefixes = (
        "audio_injector", 
        # "trainable_cond_mask", 
        # "frame_packer"
    )
    
    # 2. å®¹å™¨åˆå§‹åŒ–
    high_lr_params = []
    low_lr_params = []
    
    # ç»Ÿè®¡ç”¨å˜é‡
    stats = {
        "high_lr_count": 0,    # é«˜å­¦ä¹ ç‡å‚æ•°ä¸ªæ•°
        "low_lr_count": 0,     # ä½å­¦ä¹ ç‡å‚æ•°ä¸ªæ•° (Backboneä¸­åŸæœ¬å¯è®­ç»ƒçš„)
        "frozen_skipped": 0,   # è¢«è·³è¿‡çš„å†»ç»“å‚æ•° (å¦‚ TextEncoder)
        "zero_value_count": 0, # å®é™…å€¼ä¸º0çš„å‚æ•°ä¸ªæ•°
        "total_params": 0
    }

    # 3. éå†æ¨¡å‹æ‰€æœ‰å‚æ•°
    for name, param in model.named_parameters():
        stats["total_params"] += 1
        
        # åˆ¤æ–­æ˜¯å¦å±äºç›®æ ‡æ¨¡å— (Audio/Mask/Packer)
        is_target_module = any(prefix in name for prefix in target_prefixes)
        
        if is_target_module:
            # ============================================
            # A. ç›®æ ‡æ¨¡å—ï¼šå¼ºåˆ¶è®­ç»ƒ + å¼ºåˆ¶ç½®é›¶ + é«˜å­¦ä¹ ç‡
            # ============================================
            param.requires_grad = True # ç¡®ä¿å¼€å¯
            
            # æ‰§è¡Œå…¨é‡ç½®é›¶
            # with torch.no_grad():
            #     param.zero_()
            
            high_lr_params.append(param)
            stats["high_lr_count"] += 1
            
            # éªŒè¯ç½®é›¶
            # if param.sum() == 0: #! bug:è¿™ä¸ªæ¡ä»¶å¯¹äºæµ®ç‚¹æ•°ä¸å¤Ÿç¨³å®š
            if torch.allclose(param, torch.zeros_like(param)):
                stats["zero_value_count"] += 1
                
        else:
            # ============================================
            # B. éç›®æ ‡æ¨¡å—ï¼šå°Šé‡åŸçŠ¶æ€ (åªæ”¶å½•æœ¬æ¥å°±å¼€äº†æ¢¯åº¦çš„)
            # ============================================
            if param.requires_grad:
                # åŸæœ¬å°±æ˜¯å¯è®­ç»ƒçš„ (æ¯”å¦‚ Backbone çš„ Attention) -> ä½å­¦ä¹ ç‡
                low_lr_params.append(param)
                stats["low_lr_count"] += 1
            else:
                # åŸæœ¬å°±æ˜¯å†»ç»“çš„ (æ¯”å¦‚ Text Encoder) -> è·³è¿‡ï¼Œä¸è¿›ä¼˜åŒ–å™¨
                stats["frozen_skipped"] += 1

    # # 4. æ‰“å°è¯¦ç»†ç»Ÿè®¡æŠ¥å‘Š
    # print(f"\nğŸ“Š å‚æ•°ç»Ÿè®¡æŠ¥å‘Š:")
    # print(f"   -------------------------------------------")
    # print(f"   [Total] æ¨¡å‹æ€»å‚æ•°å±‚æ•°: {stats['total_params']}")
    # print(f"   -------------------------------------------")
    # print(f"   ğŸ”¥ [High LR Group] (Target Modules, lr={target_lr})")
    # print(f"       - åŒ…å«: {target_prefixes}")
    # print(f"       - æ•°é‡: {stats['high_lr_count']}")
    # print(f"       - ç½®é›¶éªŒè¯: {stats['zero_value_count']} / {stats['high_lr_count']} (åº”ç›¸ç­‰)")
    
    # print(f"   â„ï¸ [Low LR Group] (Backbone SFT, lr={base_lr})")
    # print(f"       - æ•°é‡: {stats['low_lr_count']}")
    # print(f"       - è¯´æ˜: è¿™äº›æ˜¯SFTæƒé‡ä¸­åŸæœ¬å¼€å¯æ¢¯åº¦çš„éƒ¨åˆ†")
    
    # print(f"   ğŸ§Š [Skipped/Frozen] (Not Training)")
    # print(f"       - æ•°é‡: {stats['frozen_skipped']}")
    # print(f"       - è¯´æ˜: è¿™äº›å‚æ•°ä¿æŒå†»ç»“ï¼Œä¸æ¶ˆè€—æ˜¾å­˜å­˜æ¢¯åº¦ (å¦‚TextEncoder)")
    # print(f"   -------------------------------------------")

    # 5. æ„å»ºä¼˜åŒ–å™¨æ‰€éœ€çš„å‚æ•°ç»„åˆ—è¡¨
    optimizer_grouped_parameters = [
        {
            "params": low_lr_params, 
            "lr": base_lr,
            "name": "backbone_low_lr"
        },
        {
            "params": high_lr_params, 
            "lr": target_lr,
            "name": "audio_new_high_lr"
        }
    ]
    
    return optimizer_grouped_parameters

def launch_training_task(
    accelerator: Accelerator,
    dataset: torch.utils.data.Dataset,
    model: DiffusionTrainingModule,
    model_logger: ModelLogger,
    learning_rate: float = 1e-5,
    weight_decay: float = 1e-2,
    num_workers: int = 1,
    save_steps: int = None,
    num_epochs: int = 1,
    args = None,
):
    if args is not None:
        # small_lr_rate = 1e-5
        learning_rate = args.learning_rate
        weight_decay = args.weight_decay
        num_workers = args.dataset_num_workers
        save_steps = args.save_steps
        num_epochs = args.num_epochs
        debug = args.debug
    
    if debug:
        diagnose_default_training_status(model)
    # optimizer = torch.optim.AdamW(model.trainable_modules(), lr=learning_rate, weight_decay=weight_decay)
    optimizer_grouped_parameters = prepare_model_and_optimizer_groups(
        model, 
        base_lr=1e-5, 
        target_lr=learning_rate
    )
    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, weight_decay=weight_decay)
    scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer)
    dataloader = torch.utils.data.DataLoader(dataset, shuffle=False, collate_fn=lambda x: x[0], num_workers=num_workers) if debug else torch.utils.data.DataLoader(dataset, shuffle=True, collate_fn=lambda x: x[0], num_workers=num_workers)
    
    model, optimizer, dataloader, scheduler = accelerator.prepare(model, optimizer, dataloader, scheduler)

    if debug:
        model_logger.on_training_start(accelerator, model)
        log_name = f"perf_debug_{accelerator.process_index}_{datetime.now().strftime('%m%d_%H%M')}.log"
        timer = DetailedStepTimer(log_file=log_name, warmup_steps=2)
        
        for epoch_id in range(num_epochs):
            end_time = time.perf_counter()
            
            for step_index, data in enumerate(tqdm(dataloader, desc=f"Epoch {epoch_id}", 
                                                    disable=not accelerator.is_main_process)):
                if step_index > 14:
                    break
                # for i, group in enumerate(optimizer.param_groups):
                #     print(f"Group {i} ({group.get('name', 'unnamed')}): lr = {group['lr']}")
                
                timer.current_step = step_index
                
                # 1. æ•°æ®åŠ è½½æ—¶é—´ï¼ˆCPU ç«¯ï¼‰
                data_load_time = time.perf_counter() - end_time
                timer.record("data_loading", data_load_time)
                
                # 2. æ•°æ®ä¼ è¾“åˆ° GPU çš„æ—¶é—´ï¼ˆå¦‚æœæ•°æ®è¿˜åœ¨ CPUï¼‰
                with timer.time_step("data_to_gpu"):
                    # å¦‚æœä½ çš„ data éœ€è¦æ‰‹åŠ¨ç§»åŠ¨åˆ° GPU
                    # data = {k: v.to(accelerator.device) for k, v in data.items()}
                    pass  # Accelerate é€šå¸¸å·²ç»å¤„ç†äº†è¿™ä¸ª
                
                with accelerator.accumulate(model):
                    
                    with timer.time_step("zero_grad"):
                        optimizer.zero_grad()
                    
                    with timer.time_step("forward"):
                        loss = model(data)
                    
                    # === å…³é”®ï¼šåˆ†ç¦» backward è®¡ç®—å’Œæ¢¯åº¦åŒæ­¥ ===
                    if accelerator.sync_gradients:
                        # è¿™æ˜¯æ¢¯åº¦ç´¯ç§¯çš„æœ€åä¸€æ­¥ï¼Œä¼šè§¦å‘ AllReduce
                        
                        # æ–¹æ³• Aï¼šç®€å•æµ‹é‡æ€»æ—¶é—´ï¼ˆæ¨èå…ˆç”¨è¿™ä¸ªï¼‰
                        with timer.time_step("backward_total"):
                            accelerator.backward(loss)
                        
                        # æ–¹æ³• Bï¼šå°è¯•åˆ†ç¦»è®¡ç®—å’ŒåŒæ­¥ï¼ˆéœ€è¦ no_sync æ”¯æŒï¼‰
                        # timer.time_backward_separated(accelerator, model, loss)
                        
                        # æµ‹é‡è¿›ç¨‹é—´ç­‰å¾…ï¼ˆè´Ÿè½½ä¸å‡è¡¡æŒ‡æ ‡ï¼‰
                        timer.measure_load_imbalance(accelerator, "pre_step_barrier")
                        
                        with timer.time_step("optimizer_step"):
                            optimizer.step()
                        
                        with timer.time_step("model_logger"):
                            model_logger.on_step_end(accelerator, model, save_steps)
                        
                        with timer.time_step("scheduler_step"):
                            scheduler.step()
                            
                    else:
                        # æ¢¯åº¦ç´¯ç§¯ä¸­é—´æ­¥ï¼Œä¸åŒæ­¥
                        with timer.time_step("backward_no_sync"):
                            accelerator.backward(loss)
                        
                        timer.record("pre_step_barrier", 0)
                        timer.record("optimizer_step", 0)
                        timer.record("model_logger", 0)
                        timer.record("scheduler_step", 0)
                
                end_time = time.perf_counter()
        
        accelerator.wait_for_everyone()
        timer.print_summary(accelerator)
        model_logger.on_training_end(accelerator, model, save_steps)
    else:
        model_logger.on_training_start(accelerator, model)
    
        for epoch_id in range(num_epochs):
            for data in tqdm(dataloader):
                with accelerator.accumulate(model):
                    optimizer.zero_grad() # PyTorch é»˜è®¤ä¼šç´¯ç§¯æ¢¯åº¦ï¼Œæ‰€ä»¥æ¯æ¬¡è¿­ä»£å¼€å§‹è¦æ‰‹åŠ¨æ¸…é›¶
                    loss = model(data) 
                    accelerator.backward(loss) # è®¡ç®— loss å¯¹æ‰€æœ‰å¯è®­ç»ƒå‚æ•°çš„æ¢¯åº¦,DLçš„æ ¸å¿ƒâ€”â€”é“¾å¼æ³•åˆ™æ±‚å¯¼
                    optimizer.step() # æ ¹æ®æ¢¯åº¦æ›´æ–°æ¨¡å‹æƒé‡, æ–°æƒé‡ = æ—§æƒé‡ - å­¦ä¹ ç‡ Ã— æ¢¯åº¦
                    model_logger.on_step_end(accelerator, model, save_steps)
                    scheduler.step() # éšç€è®­ç»ƒè¿›è¡Œï¼Œè°ƒæ•´å­¦ä¹ ç‡ï¼ˆé€šå¸¸æ˜¯é€æ¸å‡å°ï¼‰
        accelerator.wait_for_everyone() # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹å®Œæˆ
        model_logger.on_training_end(accelerator, model, save_steps)


def launch_data_process_task(
    accelerator: Accelerator,
    dataset: torch.utils.data.Dataset,
    model: DiffusionTrainingModule,
    model_logger: ModelLogger,
    num_workers: int = 8,
    args = None,
):
    if args is not None:
        num_workers = args.dataset_num_workers
        
    dataloader = torch.utils.data.DataLoader(dataset, shuffle=False, collate_fn=lambda x: x[0], num_workers=num_workers)
    model, dataloader = accelerator.prepare(model, dataloader)
    
    for data_id, data in enumerate(tqdm(dataloader)):
        with accelerator.accumulate(model):
            with torch.no_grad():
                folder = os.path.join(model_logger.output_path, str(accelerator.process_index))
                os.makedirs(folder, exist_ok=True)
                save_path = os.path.join(model_logger.output_path, str(accelerator.process_index), f"{data_id}.pth")
                data = model(data)
                torch.save(data, save_path)
